
R version 4.5.0 (2025-04-11) -- "How About a Twenty-Six"
Copyright (C) 2025 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> #!/usr/bin/R
> ## Senan Hogan-Hennessy, 6 May 2025
> ## Identifying ADE + AIE with a sem-parametric control function
> ## see Hogan-Hennessy (2025).
> 
> # Show the date:
> print(format(Sys.time(), "%H:%M %Z %A, %d %B %Y"))
[1] "19:37 EDT Tuesday, 17 June 2025"
> 
> ## Load libraries
> # Functions for data manipulation and visualisation
> library(tidyverse)
── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
✔ dplyr     1.1.4     ✔ readr     2.1.5
✔ forcats   1.0.0     ✔ stringr   1.5.1
✔ ggplot2   3.5.2     ✔ tibble    3.2.1
✔ lubridate 1.9.4     ✔ tidyr     1.3.1
✔ purrr     1.0.4     
── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
✖ dplyr::filter() masks stats::filter()
✖ dplyr::lag()    masks stats::lag()
ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors
> # Package for more distributions to sample from.
> library(MASS)

Attaching package: ‘MASS’

The following object is masked from ‘package:dplyr’:

    select

> # Package forsemi-parametric CF by splines, and one for semi-p MTEs.
> library(mgcv)
Loading required package: nlme

Attaching package: ‘nlme’

The following object is masked from ‘package:dplyr’:

    collapse

This is mgcv 1.9-1. For overview type 'help("mgcv-package")'.
> library(ivmte)
> # Semi-parametric binary choice model (for mediator first-stage).
> # (Klein Spady 1993, https://github.com/henrykye/semiBRM)
> library(semiBRM)
> 
> 
> ## Set up the R environment
> set.seed(47)
> # Define number of digits in tables and graphs
> digits.no <- 3
> # Define where output files go.
> output.folder <- file.path("sim-output")
> # Set the options for the plot sizes, in saving ggplot output.
> fig.height <- 10
> fig.width <- fig.height
> 
> # Define the sample size to work with.
> sample.N <- 10^3
> 
> 
> ################################################################################
> ## Define a function to simulate data in the triangular system.
> 
> # Define a function to simulate all observed + unobserved data 
> roy.data <- function(rho, sigma_0, sigma_1, sigma_C,
+         error.dist = "normal", sample.size = sample.N){
+     ### Inputs:
+     ## X, a matrix of covariates, continuous or binary values.
+     ## rho \in [-1, +1] measuring correlation between U_0, U_1.
+     ## sigma_0 >= 0 measuring standard deviation of U_0.
+     ## sigma_1 >= 0 measuring standard deviation of U_1.
+     ## sigma_C >= 0 measuring standard deviation of U_C.
+     ## sample.size: integer, representing output sample size (i.e., N).
+     # First covariate, \vec X_i^-
+     X_minus <- 4 + rnorm(sample.size, mean = 0, sd = 1)
+     # Second covariate (instrument for the control function).
+     X_IV <- runif(sample.size, -1, 1)
+     # Simulate the unobserved error terms.
+     mu.vector <- c(0, 0, 0)
+     sigma.matrix <- matrix(c(
+         sigma_0^2,               rho * sigma_0 * sigma_1, 0,
+         rho * sigma_0 * sigma_1, sigma_1^2,               0,
+         0,                       0,                       sigma_C^2),
+         ncol = 3)
+     # Simulate a joint normal, from which a prob integral transform can get
+     # any other (potentially correlated) distribution. 
+     U_all <- mvrnorm(n = sample.size, mu = mu.vector, sigma.matrix,
+         empirical = FALSE)
+     # Get the desired distribution.
+     if (error.dist == "normal"){
+         U_0 <- U_all[, 1]
+         U_1 <- U_all[, 2]
+         U_C <- U_all[, 3]
+     } 
+     else if (error.dist == "uniform"){
+         # uniform errors (centred on zero), with SD = sigma_0,1,C.
+         U_0 <- (2 * pnorm(U_all[, 1], mean = 0, sd = sigma_0) - 1) * sigma_0 * sqrt(3)
+         U_1 <- (2 * pnorm(U_all[, 2], mean = 0, sd = sigma_1) - 1) * sigma_1 * sqrt(3)
+         U_C <- (2 * pnorm(U_all[, 3], mean = 0, sd = sigma_C) - 1) * sigma_C * sqrt(3)
+     }
+     else { stop("Choose an error.dist from `normal`, `uniform`.")
+     }
+     # Define the mean potential outcomes.
+     mu_outcome_z_d_X <- function(z, d, x_minus){
+         return(x_minus + (z + d + z * d))
+     }
+     mu_cost_z_X <- function(z, x_minus, x_iv){
+         return(- 3 * z + x_minus - x_iv)
+     }
+     # Mean outcomes: Y_i(z, d) = mu_d(z; X_i) + U_D
+     Y_0_0 <- mu_outcome_z_d_X(0, 0, X_minus) + U_0
+     Y_0_1 <- mu_outcome_z_d_X(0, 1, X_minus) + U_1
+     Y_1_0 <- mu_outcome_z_d_X(1, 0, X_minus) + U_0
+     Y_1_1 <- mu_outcome_z_d_X(1, 1, X_minus) + U_1
+     # Roy selection: D_i(z) = 1{ Y(z, 1) - Y(z, 0) >= C_i }
+     D_0 <- as.integer(Y_0_1 - Y_0_0 >= mu_cost_z_X(0, X_minus, X_IV) + U_C)
+     D_1 <- as.integer(Y_1_1 - Y_1_0 >= mu_cost_z_X(1, X_minus, X_IV) + U_C)
+     # Observed outcomes: treatment Z
+     probZ <- 0.5
+     Z <- rbinom(sample.size, 1, probZ)
+     # Observed outcomes: mediator D
+     D <- (Z * D_1) + ((1 - Z) * D_0)
+     # Observed outcomes: outcome Y
+     Y <- (Z * D * Y_1_1) +
+         (Z * (1 - D) * Y_1_0) +
+         ((1 - Z) * D * Y_0_1) +
+         ((1 - Z) * (1 - D) * Y_0_0)
+     # Put these data to a coherent data frame.
+     combined.data <- data.frame(
+         # Observed data
+         Z, D, Y,  X_minus, X_IV,
+         # Unobserved, potential outcomes and compliance.
+         D_0, D_1,
+         Y_0_0, Y_0_1, Y_1_0, Y_1_1,
+         #mu0_Z0_X, mu1_Z0_X, mu0_Z1_X, mu1_Z1_X, muC_Z0_X, muC_Z1_X, 
+         U_0, U_1, U_C)
+     # Return the simulated data as a data frame.
+     return(combined.data)
+ }
> 
> 
> ################################################################################
> ## Define a function to show the theoretical values for the data.
> theoretical.values <- function(sim.data, digits.no = 3, print.truth = FALSE){
+     ### Inputs:
+     ## sim.data, a data frame simulated from above.
+     # Extract the potentials from simulated data.
+     Z <- sim.data$Z
+     D <- sim.data$D
+     Y <- sim.data$Y
+     X_minus <- sim.data$X_minus
+     X_IV <- sim.data$X_IV
+     D_0 <- sim.data$D_0
+     D_1 <- sim.data$D_1
+     Y_0_0 <- sim.data$Y_0_0
+     Y_0_1 <- sim.data$Y_0_1
+     Y_1_0 <- sim.data$Y_1_0
+     Y_1_1 <- sim.data$Y_1_1
+     U_0 <- sim.data$U_0
+     U_1 <- sim.data$U_1
+     U_C <- sim.data$U_C
+     # Get the true first-stage effects
+     first_stage <- D_1 - D_0
+     average_first_stage <- mean(first_stage)
+     # Get the theoretical total effect/reduced form/ATE
+     total_effect <-
+         (Y_1_1 - Y_0_0) * (D_1 == 1 & D_0 == 0) +
+         (Y_1_1 - Y_0_1) * (D_1 == 1 & D_0 == 1) +
+         (Y_1_0 - Y_0_0) * (D_1 == 0 & D_0 == 0)
+     average_total_effect <- mean(total_effect)
+     # Get the theoretical indirect effect.
+     indirect_effect <-
+         (Z * (Y_1_1 - Y_1_0) + (1 - Z) * (Y_0_1 - Y_0_0))
+     controlled_indirect_effect <- mean(indirect_effect)
+     average_indirect_effect <- mean((D_1 == 1 & D_0 == 0) * indirect_effect)
+     # Get the theoretical direct effect.
+     direct_effect <- (D * (Y_1_1 - Y_0_1) + (1 - D) * (Y_1_0 - Y_0_0))
+     average_direct_effect <- mean(direct_effect)
+     # Show the real underlying values.
+     if (print.truth == TRUE){
+         print("Here is a summary of the (unobserved) true effects:")
+         # Show how many ATs, NTs, Compliers in terms of D_i(Z) for Z = 0, 1.
+         print("How many mediator compliers in the system?")
+         print(table(D_1, D_0) / NROW(sim.data))
+         print("How many actually took the mediator, i.e. Pr(D = 1)?")
+         print(mean(D))
+         # Show the real treatment effects
+         print(paste0(c("The average total effect:",    as.numeric(average_total_effect))))
+         print(paste0(c("The average first-stage:",     as.numeric(average_first_stage))))
+         print(paste0(c("The average direct effect:",   as.numeric(average_direct_effect))))
+         print(paste0(c("The average indirect effect:", as.numeric(average_indirect_effect))))
+         print(paste0(c("Cov(V, U_1):", cov(U_C - U_1 + U_0, U_0))))
+         print(paste0(c("Cov(V, U_0):", cov(U_C - U_1 + U_0, U_1))))
+     }
+     # Define a named list to return
+     output.list <- list(
+         average_first_stage     = average_first_stage,
+         average_total_effect    = average_total_effect,
+         average_direct_effect   = average_direct_effect,
+         average_indirect_effect = average_indirect_effect)
+     # Return the output.list
+     return(output.list)
+ }
> 
> 
> ################################################################################
> ## Define a function to estimate mediation, given the first + second-stages.
> 
> # Estimate the values, given a first and second-stages
> estimated.values <- function(firststage.reg, secondstage.reg, totaleffect.reg,
+     example.data, complier.adjustment = NULL){
+     ### Inputs:
+     ## example.data, a data frame simulated from above.
+     example_Z0.data <- example.data
+     example_Z1.data <- example.data
+     example_D0.data <- example.data
+     example_D1.data <- example.data
+     example_Z0.data$Z <- 0
+     example_Z1.data$Z <- 1
+     example_D0.data$D <- 0
+     example_D1.data$D <- 1
+     # calculate the first-stage by prediction
+     firststage.est <- predict(
+         firststage.reg, newdata = example_Z1.data, type = "response") - predict(
+             firststage.reg, newdata = example_Z0.data, type = "response")
+     # Calculate the total effect estimate by prediction.
+     totaleffect.est <- predict(
+         totaleffect.reg, newdata = example_Z1.data) - predict(
+             totaleffect.reg, newdata = example_Z0.data)
+     # calculate the second-stage direct effect
+     direct.est <- predict(
+         secondstage.reg, newdata = example_Z1.data) - predict(
+             secondstage.reg, newdata = example_Z0.data)
+     # calculate the second-stage (controlled) indirect effect
+     indirect.est <- predict(
+         secondstage.reg, newdata = example_D1.data) - predict(
+             secondstage.reg, newdata = example_D0.data)
+     # Add the Kline Walters (2019) IV-type complier adjustment (provided externally).
+     if (!is.null(complier.adjustment)) {
+         indirect.est <- indirect.est + complier.adjustment
+     }
+     # Return the mean estimates.
+     output.list <- list(
+         "first-stage"     = mean(firststage.est, na.rm = TRUE),
+         "total-effect"    = mean(totaleffect.est, na.rm = TRUE),
+         "direct-effect"   = mean(direct.est, na.rm = TRUE),
+         "indirect-effect" = mean(firststage.est * indirect.est, na.rm = TRUE))
+     # Return the output.list
+     return(output.list)
+ }
> 
> 
> ################################################################################
> ## Two mediation estimation functions (1) Parametric CF (2) Semi-parametric CF.
> 
> # Define a function to Heckman selection correct mediation est, in two-stages.
> mediate.heckit <- function(example.data){
+     example_Z0.data <- example.data
+     example_Z1.data <- example.data
+     example_D0.data <- example.data
+     example_D1.data <- example.data
+     example_Z0.data$Z <- 0
+     example_Z1.data$Z <- 1
+     example_D0.data$D <- 0
+     example_D1.data$D <- 1
+     # 0. Total effect regression.
+     totaleffect.reg <- lm(Y ~ 1 + Z + X_minus,
+         data = example.data)
+     # 1. Probit first-stage (well identified).
+     heckit_firststage.reg <- glm(D ~ (1 + Z) + X_IV + X_minus,
+         family = binomial(link = "probit"),
+         data = example.data)
+     # 2. Define the CFs --- for assumed N(0,1) dist.
+     lambda_1.fun <- function(pi.est){
+         # Inv Mills ratio, taking as input the estimated mediator propensity.
+         return(dnorm(qnorm(pi.est)) / pnorm(qnorm(pi.est)))
+     }
+     pi.est <- predict(heckit_firststage.reg, type = "response")
+     example.data$lambda_0 <- (1 - example.data$D) * lambda_1.fun(pi.est) * (
+         - pi.est / (1 - pi.est))
+     example.data$lambda_1 <- example.data$D * lambda_1.fun(pi.est)
+     # 3. Estimate second-stage, including the CFs.
+     heckit_secondstage.reg <- lm(Y ~ (1 + Z * D) + X_minus + lambda_0 + lambda_1,
+         data = example.data)
+     # Compensate complier difference in AIE, by Kline Walters (2019) IV-type adjustment.
+     pi_0.est <- predict(heckit_firststage.reg, newdata = example_Z0.data, type = "response")
+     pi_1.est <- predict(heckit_firststage.reg, newdata = example_Z1.data, type = "response")
+     Gamma.big <-  (pi_1.est * lambda_1.fun(pi_1.est) - pi_0.est * lambda_1.fun(pi_0.est)) / (
+         pi_1.est - pi_0.est)
+     rho_0 <- coef(summary(heckit_secondstage.reg))["lambda_0", "Estimate"]
+     rho_1 <- coef(summary(heckit_secondstage.reg))["lambda_1", "Estimate"]
+     complier.adjustment <- (rho_1 - rho_0) * Gamma.big
+     # Compile the estimates.
+     heckit.est <- estimated.values(heckit_firststage.reg, heckit_secondstage.reg,
+         totaleffect.reg, example.data,
+         complier.adjustment = complier.adjustment)
+     # Return the off-setting estimates.
+     output.list <- list(
+         "first-stage"     = heckit.est$"first-stage",
+         "total-effect"    = heckit.est$"total-effect",
+         "direct-effect"   = heckit.est$"direct-effect",
+         "indirect-effect" = heckit.est$"indirect-effect")
+     return(output.list)
+ }
> 
> # Define a function to two-stage semi-parametric CF for CM effects.
> mediate.semiparametric <- function(example.data){
+     example_Z0.data <- example.data
+     example_Z1.data <- example.data
+     example_D0.data <- example.data
+     example_D1.data <- example.data
+     example_Z0.data$Z <- 0
+     example_Z1.data$Z <- 1
+     example_D0.data$D <- 0
+     example_D1.data$D <- 1
+     # 1. Total effect regression.
+     totaleffect.reg <- lm(Y ~ 1 + Z + X_minus,
+         data = example.data)
+     totaleffect.est <- mean(predict(
+         totaleffect.reg, newdata = example_Z1.data) - predict(
+             totaleffect.reg, newdata = example_Z0.data))
+     # 2. Semi-parametric first-stage
+     cf_firststage.reg <- semiBRM(D ~ 1 + Z + X_IV + X_minus,
+         data = example.data, control = list(iterlim = 100))
+     example.data$pi.est <- predict(cf_firststage.reg, type = "response")$prob
+     pi_0.est <- predict(cf_firststage.reg,
+         newdata = example_Z0.data, type = "response")$prob
+     pi_1.est <- predict(cf_firststage.reg,
+         newdata = example_Z1.data, type = "response")$prob
+     pi.bar <- mean(pi_1.est - pi_0.est)
+     # 3. Semi-parametric series estimation of the second-stage.
+     cf_secondstage_D0.reg <- gam(Y ~ 1 + Z + X_minus + s(pi.est, bs = "cr"),
+         method = "REML",
+         data = example.data, subset = (D == 0))
+     cf_secondstage_D1.reg <- gam(Y ~ 1 + Z + X_minus + s(pi.est, bs = "cr"),
+         method = "REML",
+         data = example.data, subset = (D == 1))
+     # 4. Compose the CM effects from this object.
+     D_0 <- 1 - mean(example.data$D)
+     D_1 <- 1 - D_0
+     # 4.1 ADE point estimate, from the CF model.
+     gammma.est <- coef(cf_secondstage_D0.reg)["Z"]
+     delta_plus.est <- coef(cf_secondstage_D1.reg)["Z"]
+     ade.est <- as.numeric(D_0 * gammma.est + D_1 * delta_plus.est)
+     # 4.2 AIE by using ADE estimate, relative to ATE.
+     # (Avoiding semi-parametric extrapolation, see notes on ATE comparison)
+     delta.est <- delta_plus.est - gammma.est
+     ade_Z0.est <- gammma.est + delta.est * mean(
+         example.data$D[example.data$Z == 0])
+     ade_Z1.est <- gammma.est + delta.est * mean(
+         example.data$D[example.data$Z == 1])
+     aie.est <- totaleffect.est - mean(
+         (1 - example.data$Z) * ade_Z1.est + example.data$Z * ade_Z0.est)
+     # Return the estimates.
+     output.list <- list(
+         "first-stage"     = pi.bar,
+         "total-effect"    = totaleffect.est,
+         "direct-effect"   = ade.est,
+         "indirect-effect" = aie.est)
+     return(output.list)
+ }
> 
> 
> ################################################################################
> ## Define a function to automate the DGP generation, or bootstrapping models.
> 
> # Define a loop.
> estimated.loop <- function(boot.reps, example.data,
+         bootstrap = TRUE, print.progress = FALSE, error.dist = "normal",
+         # DGP parameters
+         rho, sigma_0, sigma_1, sigma_C) {
+     # Define lists the will be returned:
+     # 1. Naive OLS (fine for total effect, but not CM effects).
+     ols_total_effect <- c()
+     ols_direct_effect <- c()
+     ols_indirect_effect <- c()
+     # 2. Heckman selection model adjustment
+     heckit_direct_effect <- c()
+     heckit_indirect_effect <- c()
+     # 3. Semi-parametric Control function.
+     cf_direct_effect <- c()
+     cf_indirect_effect <- c()
+     # Calculate the truth values, given the input data
+     truth_total_effect <- c()
+     truth_direct_effect <- c()
+     truth_indirect_effect <- c()
+     truth.est <- theoretical.values(example.data)
+     ## Loop across the bootstraps values.
+     for (i in seq(1, boot.reps)){
+         # If bootstrapping, just resample from provided data.
+         if (bootstrap == TRUE) {
+             boot.indicies <- sample(
+                 seq(1, NROW(example.data)), NROW(example.data), replace = TRUE)
+             boot.data <- example.data[boot.indicies, ]
+         }
+         # If a regular DGP re-simulation, get new data.
+         else if (bootstrap == FALSE){
+             boot.data <- roy.data(rho, sigma_0, sigma_1, sigma_C,
+                 error.dist = error.dist)
+             # Update the truth values to the newest simulated data, if so.
+             truth.est <- theoretical.values(boot.data)
+         }
+         else {stop("The `bootstrap' option only takes values of TRUE or FALSE.")}
+         # Print, if want the consol output of how far we are.
+         if (print.progress == TRUE){
+             if ((100 * (i / boot.reps)) %% 1 == 0) {
+                 print(paste0(i, " out of ", boot.reps, ", ", 100 * (i / boot.reps), "% done."))
+             }
+         }
+         # First, get the total effect, by a standard regression.
+         totaleffect.reg <- lm(Y ~ 1 + Z + X_minus, data = boot.data)
+         # Now get the mediation effects, by different approaches.
+         # 2. OLS estimate of second-stage
+         ols_firststage.reg <- lm(D ~ (1 + Z) + X_minus + X_IV, data = boot.data)
+         ols_secondstage.reg <- lm(Y ~ 1 + Z * D + X_minus, data = boot.data)
+         ols.est <- estimated.values(ols_firststage.reg, ols_secondstage.reg,
+             totaleffect.reg, boot.data)
+         # 3. Heckman-style selection-into-mediator model estimates.
+         heckit.est <- mediate.heckit(boot.data)
+         # 4. CF / MTE semi-parametric selection-into-mediator model estimates.
+         cf.est <- mediate.semiparametric(boot.data)
+         # Save the outputs.
+         truth_total_effect[i]     <- truth.est$average_total_effect
+         truth_direct_effect[i]    <- truth.est$average_direct_effect
+         truth_indirect_effect[i]  <- truth.est$average_indirect_effect
+         ols_total_effect[i]       <- ols.est$"total-effect"
+         ols_direct_effect[i]      <- ols.est$"direct-effect"
+         ols_indirect_effect[i]    <- ols.est$"indirect-effect"
+         heckit_direct_effect[i]   <- heckit.est$"direct-effect"
+         heckit_indirect_effect[i] <- heckit.est$"indirect-effect"
+         cf_direct_effect[i]       <- cf.est$"direct-effect"
+         cf_indirect_effect[i]     <- cf.est$"indirect-effect"
+     }
+     # Return the bootstrap data.
+     output.list <- list()
+     output.list$data <- data.frame(
+         # Total effect
+         truth_total_effect     = truth_total_effect,
+         ols_total_effect       = ols_total_effect,
+         # Direct effect
+         truth_direct_effect    = truth_direct_effect,
+         ols_direct_effect      = ols_direct_effect,
+         heckit_direct_effect   = heckit_direct_effect,
+         cf_direct_effect       = cf_direct_effect,
+         # Indirect effect
+         truth_indirect_effect  = truth_indirect_effect,
+         ols_indirect_effect    = ols_indirect_effect,
+         heckit_indirect_effect = heckit_indirect_effect,
+         cf_indirect_effect     = cf_indirect_effect)
+     # Calculate the needed statistics, to return
+     output.list$estimates <- data.frame(
+         # Truth
+         truth_direct_effect     = as.numeric(mean(truth_direct_effect)),
+         truth_indirect_effect   = as.numeric(mean(truth_indirect_effect)),
+         # OLS mean, and the 95% confidence intervals
+         ols_direct_effect       = as.numeric(mean(ols_direct_effect)),
+         ols_direct_effect_se    = as.numeric(sd(ols_direct_effect)),
+         ols_direct_effect_up    = as.numeric(quantile(ols_direct_effect,
+             probs = 0.975, na.rm = TRUE)),
+         ols_direct_effect_low   = as.numeric(quantile(ols_direct_effect,
+             probs = 0.025, na.rm = TRUE)),
+         ols_indirect_effect     = as.numeric(mean(ols_indirect_effect)),
+         ols_indirect_effect_se  = as.numeric(sd(ols_indirect_effect)),
+         ols_indirect_effect_up  = as.numeric(quantile(ols_indirect_effect,
+             probs = 0.975, na.rm = TRUE)),
+         ols_indirect_effect_low = as.numeric(quantile(ols_indirect_effect,
+             probs = 0.025, na.rm = TRUE)),
+         # Control Fun mean, and the 95% confidence intervals
+         heckit_direct_effect        = as.numeric(mean(heckit_direct_effect)),
+         heckit_direct_effect_se     = as.numeric(sd(heckit_direct_effect)),
+         heckit_direct_effect_up     = as.numeric(quantile(heckit_direct_effect,
+             probs = 0.975, na.rm = TRUE)),
+         heckit_direct_effect_low    = as.numeric(quantile(heckit_direct_effect,
+             probs = 0.025, na.rm = TRUE)),
+         heckit_indirect_effect      = as.numeric(mean(heckit_indirect_effect)),
+         heckit_indirect_effect_se   = as.numeric(sd(heckit_indirect_effect)),
+         heckit_indirect_effect_up   = as.numeric(quantile(heckit_indirect_effect,
+             probs = 0.975, na.rm = TRUE)),
+         heckit_indirect_effect_low  = as.numeric(quantile(heckit_indirect_effect,
+             probs = 0.025, na.rm = TRUE)),
+         # Control Fun mean, and the 95% confidence intervals
+         cf_direct_effect        = as.numeric(mean(cf_direct_effect)),
+         cf_direct_effect_se     = as.numeric(sd(cf_direct_effect)),
+         cf_direct_effect_up     = as.numeric(quantile(cf_direct_effect,
+             probs = 0.975, na.rm = TRUE)),
+         cf_direct_effect_low    = as.numeric(quantile(cf_direct_effect,
+             probs = 0.025, na.rm = TRUE)),
+         cf_indirect_effect      = as.numeric(mean(cf_indirect_effect)),
+         cf_indirect_effect_se   = as.numeric(sd(cf_indirect_effect)),
+         cf_indirect_effect_up   = as.numeric(quantile(cf_indirect_effect,
+             probs = 0.975, na.rm = TRUE)),
+         cf_indirect_effect_low  = as.numeric(quantile(cf_indirect_effect,
+             probs = 0.025, na.rm = TRUE)))
+     return(output.list)
+ }
> 
> 
> ################################################################################
> ## Compare estimation methods, in one-shot simulation.
> 
> ## Simulate the data with given rho, sigma_0, sigma_1, sigma_C values.
> rho <- 0.5
> sigma_0 <- 1
> sigma_1 <- 1.5
> sigma_C <- 0.5
> simulated.data <- roy.data(rho, sigma_0, sigma_1, sigma_C)
> # SHow the theoretical direct + indirect values
> print(theoretical.values(simulated.data, print.truth = TRUE))
[1] "Here is a summary of the (unobserved) true effects:"
[1] "How many mediator compliers in the system?"
   D_0
D_1     0     1
  0 0.310 0.000
  1 0.635 0.055
[1] "How many actually took the mediator, i.e. Pr(D = 1)?"
[1] 0.369
[1] "The average total effect:" "2.58564546338904"         
[1] "The average first-stage:" "0.635"                   
[1] "The average direct effect:" "1.369"                     
[1] "The average indirect effect:" "1.20964546338904"            
[1] "Cov(V, U_1):"      "0.330107214762375"
[1] "Cov(V, U_0):"      "-1.44271308429246"
$average_first_stage
[1] 0.635

$average_total_effect
[1] 2.585645

$average_direct_effect
[1] 1.369

$average_indirect_effect
[1] 1.209645

> 
> # Show that the regression specification holds exactly, if specified correctly.
> true_firststage.reg <- glm(D ~ (1 + Z) + X_IV + X_minus + U_C + U_0 + U_1,
+     family = binomial(link = "probit"),
+     data = simulated.data)
Warning messages:
1: glm.fit: algorithm did not converge 
2: glm.fit: fitted probabilities numerically 0 or 1 occurred 
> true_secondstage.reg <- lm(Y ~ (1 + Z * D) + X_minus +
+     # including the unobserved errors:
+     D * U_0 + D * U_1,
+     data = simulated.data)
> true_total.reg <- lm(Y ~ (1 + Z) + X_minus, data = simulated.data)
> print(theoretical.values(simulated.data))
$average_first_stage
[1] 0.635

$average_total_effect
[1] 2.585645

$average_direct_effect
[1] 1.369

$average_indirect_effect
[1] 1.209645

> print(estimated.values(true_firststage.reg, true_secondstage.reg,
+     true_total.reg, simulated.data))
$`first-stage`
[1] 0.6419995

$`total-effect`
[1] 2.557287

$`direct-effect`
[1] 1.369

$`indirect-effect`
[1] 1.225511

> # See how the first and second-stages are perfect:
> print(summary(true_firststage.reg))

Call:
glm(formula = D ~ (1 + Z) + X_IV + X_minus + U_C + U_0 + U_1, 
    family = binomial(link = "probit"), data = simulated.data)

Coefficients:
            Estimate Std. Error z value Pr(>|z|)
(Intercept)    265.6     2273.5   0.117    0.907
Z             1341.1     9427.7   0.142    0.887
X_IV           338.9     2058.3   0.165    0.869
X_minus       -317.3     2340.5  -0.136    0.892
U_C           -315.1     2713.5  -0.116    0.908
U_0           -343.1     2450.2  -0.140    0.889
U_1            333.4     2248.2   0.148    0.882

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 1.3168e+03  on 999  degrees of freedom
Residual deviance: 6.0642e-05  on 993  degrees of freedom
AIC: 14

Number of Fisher Scoring iterations: 25

> print(summary(true_secondstage.reg))

Call:
lm(formula = Y ~ (1 + Z * D) + X_minus + D * U_0 + D * U_1, data = simulated.data)

Residuals:
       Min         1Q     Median         3Q        Max 
-1.977e-14 -1.970e-15 -7.600e-16  3.100e-16  4.359e-13 

Coefficients:
              Estimate Std. Error    t value Pr(>|t|)    
(Intercept)  1.972e-14  2.419e-15  8.152e+00 1.07e-15 ***
Z            1.000e+00  1.752e-15  5.709e+14  < 2e-16 ***
D            1.000e+00  4.074e-15  2.454e+14  < 2e-16 ***
X_minus      1.000e+00  5.642e-16  1.773e+15  < 2e-16 ***
U_0          1.000e+00  8.359e-16  1.196e+15  < 2e-16 ***
U_1         -4.661e-16  5.983e-16 -7.790e-01    0.436    
Z:D          1.000e+00  4.200e-15  2.381e+14  < 2e-16 ***
D:U_0       -1.000e+00  1.385e-15 -7.222e+14  < 2e-16 ***
D:U_1        1.000e+00  1.036e-15  9.653e+14  < 2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 1.724e-14 on 991 degrees of freedom
Multiple R-squared:      1,	Adjusted R-squared:      1 
F-statistic: 1.758e+30 on 8 and 991 DF,  p-value: < 2.2e-16

> #! Note: this automatically includes the complier term in the indirect effect.
> #!       If U_0, U_1 are unobserved, this inclusion is not automatic.
> 
> # Show how the OLS result gives a biased result (if rho != 0),
> # using the same second-stage for both direct + indirect estimates.
> ols_firststage.reg <- lm(D ~ (1 + Z) + X_minus + X_IV, data = simulated.data)
> ols_secondstage.reg <- lm(Y ~ 1 + Z * D + X_minus, data = simulated.data)
> print(theoretical.values(simulated.data))
$average_first_stage
[1] 0.635

$average_total_effect
[1] 2.585645

$average_direct_effect
[1] 1.369

$average_indirect_effect
[1] 1.209645

> print(estimated.values(ols_firststage.reg, ols_secondstage.reg,
+     true_total.reg, simulated.data))
$`first-stage`
[1] 0.638796

$`total-effect`
[1] 2.557287

$`direct-effect`
[1] 1.012934

$`indirect-effect`
[1] 1.532095

> 
> # Show how the CF approaches get it correct (with complier adjustment).
> print(theoretical.values(simulated.data))
$average_first_stage
[1] 0.635

$average_total_effect
[1] 2.585645

$average_direct_effect
[1] 1.369

$average_indirect_effect
[1] 1.209645

> print(mediate.heckit(simulated.data))
$`first-stage`
[1] 0.6387428

$`total-effect`
[1] 2.557287

$`direct-effect`
[1] 1.701252

$`indirect-effect`
[1] 0.8554799

> print(mediate.semiparametric(simulated.data))
$`first-stage`
[1] 0.6204871

$`total-effect`
[1] 2.557287

$`direct-effect`
[1] 1.638965

$`indirect-effect`
[1] 0.9207335

> 
> # One-shot showing how the uniform errors screw up the Heckman selection model.
> uniform.data <- roy.data(rho, sigma_0, sigma_1, sigma_C, error.dist = "uniform")
> print(theoretical.values(uniform.data))
$average_first_stage
[1] 0.64

$average_total_effect
[1] 2.571352

$average_direct_effect
[1] 1.37

$average_indirect_effect
[1] 1.197352

> print(mediate.heckit(uniform.data))
$`first-stage`
[1] 0.6395343

$`total-effect`
[1] 2.533776

$`direct-effect`
[1] 1.321192

$`indirect-effect`
[1] 1.216899

> print(mediate.semiparametric(uniform.data))
$`first-stage`
[1] 0.6115619

$`total-effect`
[1] 2.533776

$`direct-effect`
[1] 1.484942

$`indirect-effect`
[1] 1.058266

> 
> 
> ################################################################################
> ## Plot DGP-strap results for normally distributed errors, repeatedly drawn
> # Note, using the same input parameters as before, to keep comparable.
> 
> # First, show us what huge sample gives for the truth values.
> roy.data(rho, sigma_0, sigma_1, sigma_C, sample.size = 10^6,
+     error.dist = "normal") %>%
+     theoretical.values(print.truth = TRUE) %>%
+     print()
[1] "Here is a summary of the (unobserved) true effects:"
[1] "How many mediator compliers in the system?"
   D_0
D_1        0        1
  0 0.292696 0.000000
  1 0.657430 0.049874
[1] "How many actually took the mediator, i.e. Pr(D = 1)?"
[1] 0.378719
[1] "The average total effect:" "2.5931970940726"          
[1] "The average first-stage:" "0.65743"                 
[1] "The average direct effect:" "1.378719"                  
[1] "The average indirect effect:" "1.2147380940726"             
[1] "Cov(V, U_1):"      "0.247774991786382"
[1] "Cov(V, U_0):"      "-1.49784956507516"
$average_first_stage
[1] 0.65743

$average_total_effect
[1] 2.593197

$average_direct_effect
[1] 1.378719

$average_indirect_effect
[1] 1.214738

> 
> # Base data to input.
> normal.data <- roy.data(rho, sigma_0, sigma_1, sigma_C, error.dist = "normal")
> 
> # Get dist-strapped point est for the CF approach
> sim.reps <- 10^4
> normal.est <- estimated.loop(sim.reps, normal.data,
+     bootstrap = FALSE, print.progress = TRUE,  error.dist = "normal",
+     rho, sigma_0, sigma_1, sigma_C)
[1] "100 out of 10000, 1% done."
[1] "200 out of 10000, 2% done."
[1] "300 out of 10000, 3% done."
[1] "400 out of 10000, 4% done."
[1] "500 out of 10000, 5% done."
[1] "600 out of 10000, 6% done."
[1] "800 out of 10000, 8% done."
[1] "900 out of 10000, 9% done."
[1] "1000 out of 10000, 10% done."
[1] "1100 out of 10000, 11% done."
[1] "1200 out of 10000, 12% done."
[1] "1300 out of 10000, 13% done."
[1] "1500 out of 10000, 15% done."
[1] "1600 out of 10000, 16% done."
[1] "1700 out of 10000, 17% done."
[1] "1800 out of 10000, 18% done."
[1] "1900 out of 10000, 19% done."
[1] "2000 out of 10000, 20% done."
[1] "2100 out of 10000, 21% done."
[1] "2200 out of 10000, 22% done."
[1] "2300 out of 10000, 23% done."
[1] "2400 out of 10000, 24% done."
[1] "2500 out of 10000, 25% done."
[1] "2600 out of 10000, 26% done."
[1] "2700 out of 10000, 27% done."
[1] "3000 out of 10000, 30% done."
[1] "3100 out of 10000, 31% done."
[1] "3200 out of 10000, 32% done."
[1] "3300 out of 10000, 33% done."
[1] "3400 out of 10000, 34% done."
[1] "3500 out of 10000, 35% done."
[1] "3600 out of 10000, 36% done."
[1] "3700 out of 10000, 37% done."
[1] "3800 out of 10000, 38% done."
[1] "3900 out of 10000, 39% done."
[1] "4000 out of 10000, 40% done."
[1] "4100 out of 10000, 41% done."
[1] "4200 out of 10000, 42% done."
[1] "4300 out of 10000, 43% done."
[1] "4400 out of 10000, 44% done."
[1] "4500 out of 10000, 45% done."
[1] "4600 out of 10000, 46% done."
[1] "4700 out of 10000, 47% done."
[1] "4800 out of 10000, 48% done."
[1] "4900 out of 10000, 49% done."
